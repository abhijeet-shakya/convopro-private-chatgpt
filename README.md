# ğŸ¤– Convo-Pro â€” ChatGPT-like Conversational AI App

Convo-Pro is a **ChatGPT-style conversational AI application** built using **LangChain**, **Groq LLMs**, **Streamlit**, and **MongoDB**.  
It delivers a **real-time streaming chat experience**, supports **persistent conversation history**, and follows a **modular, production-ready architecture**.

---

## ğŸš€ Features

- âš¡ **Real-time token-level streaming responses** (ChatGPT-like typing)
- ğŸ§  **Groq-powered LLM inference** via LangChain
- ğŸ’¬ **Persistent chat history** with MongoDB
- ğŸ·ï¸ **Automatic chat title generation** using LLM prompts
- ğŸ—‘ï¸ **Per-conversation delete functionality**
- ğŸ” **Multiple conversations** with sidebar navigation
- ğŸ¯ **System-prompt controlled formatting** (clean Markdown & LaTeX math)
- ğŸ§© **Modular and scalable backend design**

---

## ğŸ› ï¸ Tech Stack

- **Language:** Python  
- **LLM Framework:** LangChain  
- **LLM Provider:** Groq (LLaMA models)  
- **UI Framework:** Streamlit  
- **Database:** MongoDB (PyMongo)  

**Core Concepts:**
- Streaming responses  
- Prompt engineering  
- Session state management  
- Clean architecture & separation of concerns  

---

## ğŸ“‚ Project Structure
```
convo-pro/
â”‚
â”œâ”€â”€ config/
â”‚ â””â”€â”€ settings.py # Environment & config management
â”‚
â”œâ”€â”€ llm_factory/
â”‚ â””â”€â”€ get_llm.py # Groq LLM initialization & streaming
â”‚
â”œâ”€â”€ services/
â”‚ â”œâ”€â”€ chat_utilities.py # Chat logic (streaming, formatting)
â”‚ â”œâ”€â”€ get_title.py # Auto chat title generation
â”‚ â””â”€â”€ get_models_list.py # Available Groq models
â”‚
â”œâ”€â”€ db/
â”‚ â””â”€â”€ conversations.py # MongoDB conversation storage
â”‚
â”œâ”€â”€ main.py # Streamlit application
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/abhijeet-shakya/convopro-private-chatgpt.git
cd convo-pro
```

### 2ï¸âƒ£ Create virtual environment
```bash
python -m venv .venv
source .venv/bin/activate   # Linux / Mac
.venv\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Environment Variables
Create a `.env` file in the project root:

```env
GROQ_API_KEY=your_groq_api_key
GROQ_MODEL="llama-3.1-8b-instant,groq/compound,openai/gpt-oss-120b"

MONGO_DB_URL=mongodb://localhost:27017
MONGO_DB_NAME=convo_pro
```
### 5ï¸âƒ£ Run MongoDB
Make sure MongoDB is running locally or update the URI for a cloud instance.

### 6ï¸âƒ£ Run the application
```bash
streamlit run main.py
```


## ğŸ§  How It Works (High-Level)
- User enters a message in the Streamlit UI
- Chat history is stored in `st.session_state`
- Messages are converted into LangChain `Message` objects
- Groq LLM streams tokens in real-time
- Responses are rendered incrementally in the UI
- Conversations and messages are persisted in MongoDB

## ğŸ¯ Prompt Engineering
The app uses system prompts to control:
- Output length (token limits)
- Markdown formatting
- Concise, structured responses
This ensures clean, readable answers, especially for technical topics.


## ğŸš€ Future Enhancements
- ğŸ“š RAG (PDF / document chat)
- ğŸ” User authentication
- ğŸŒ Deployment
- ğŸ³ Docker support

## Author
**Abhijeet Shakya**
ğŸ”— GitHub: https://github.com/abhijeet-shakya








